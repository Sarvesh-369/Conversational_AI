{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Things to find:<br />\n",
        "1)Length of the document(i.e No.of terms after tokenization)(find average also)<br />\n",
        "2)Length of each section (i.e No.of terms after tokenization)(find average also)<br />\n",
        "3)How many links per document(find average also)<br />\n",
        "4)links received per section(find average also)<br />\n",
        "5)number of images for entire document)<br />\n",
        "6)number of images per section<br />\n",
        "7)number of duplicate links<br />\n",
        "8)Length of the document post pre processing\n"
      ],
      "metadata": {
        "id": "Ii5IyoqSHgtG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juitNmLFL0JV",
        "outputId": "b9b5a909-e933-4bb8-883c-d2bc9889e182"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Function to extract the length of a document\n",
        "def get_document_length(text):\n",
        "    words = word_tokenize(text)\n",
        "    return len(words)\n",
        "\n",
        "# Function to calculate the average value\n",
        "def calculate_average(values):\n",
        "    if len(values) > 0:\n",
        "      average = sum(values) / len(values)\n",
        "      return  '{:.2f}'.format(average)\n",
        "    return 0\n",
        "\n",
        "# Function to extract information from a Wikipedia link\n",
        "def extract_wikipedia_info(link):\n",
        "    response = requests.get(link)\n",
        "    html_content = response.text\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "    # Extract the main content of the page\n",
        "    main_content = soup.find(id='mw-content-text')\n",
        "    if main_content is None:\n",
        "        return None\n",
        "\n",
        "    # Extract the document length\n",
        "    document_length = get_document_length(main_content.get_text())\n",
        "\n",
        "    # Extract the length of data under each headline\n",
        "    headlines = main_content.find_all('span', {'class': 'mw-headline'})\n",
        "    headlines_length = []\n",
        "    for headline in headlines:\n",
        "        next_p = headline.find_next('p')\n",
        "        if next_p is not None:\n",
        "            headlines_length.append(get_document_length(next_p.get_text()))\n",
        "\n",
        "    # Extract the number of links per document\n",
        "    document_links = main_content.find_all('a')\n",
        "    relevant_links = [link for link in document_links if link.get('href') and not link.get('class')]\n",
        "    links_per_document = len(relevant_links)\n",
        "\n",
        "    # Extract the number of links per section\n",
        "    links_per_section = []\n",
        "    sections = main_content.find_all('h2')\n",
        "    for section in sections:\n",
        "        section_links = section.find_next_sibling('ul').find_all('a')\n",
        "        relevant_section_links = [link for link in section_links if link.get('href') and not link.get('class')]\n",
        "        links_per_section.append(len(relevant_section_links))\n",
        "\n",
        "    # Extract the number of images for the entire document\n",
        "    images = main_content.find_all('img')\n",
        "    num_images = len(images)\n",
        "\n",
        "    # Extract the number of images per section\n",
        "    images_per_section = []\n",
        "    for section in sections:\n",
        "        section_images = section.find_next_sibling('ul').find_all('img')\n",
        "        images_per_section.append(len(section_images))\n",
        "\n",
        "    # Extract the number of duplicate links\n",
        "    unique_links = set(link.get('href') for link in relevant_links)\n",
        "    num_duplicate_links = len(relevant_links) - len(unique_links)\n",
        "\n",
        "    return {\n",
        "        'Document Length': document_length,\n",
        "        'Average Length of Data per Headline': calculate_average(headlines_length),\n",
        "        'Links per Document': links_per_document,\n",
        "        'Average Links per Section': calculate_average(links_per_section),\n",
        "        'Number of Images for Entire Document': num_images,\n",
        "        'Average Images per Section': calculate_average(images_per_section),\n",
        "        'Number of Duplicate Links': num_duplicate_links\n",
        "    }\n",
        "\n",
        "# Provide the Wikipedia link for extraction\n",
        "link = 'https://en.wikipedia.org/wiki/Apple_Inc.'\n",
        "\n",
        "# Extract information from the link\n",
        "result = extract_wikipedia_info(link)\n",
        "\n",
        "# Print the results\n",
        "if result is not None:\n",
        "    for key, value in result.items():\n",
        "        print(key + ':', value)\n",
        "else:\n",
        "    print(\"Failed to extract information from the provided link.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Phq6Ns4fZicl",
        "outputId": "a9e680b3-c202-4f89-86b4-4c53edf88799"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document Length: 36222\n",
            "Average Length of Data per Headline: 101.73\n",
            "Links per Document: 3363\n",
            "Average Links per Section: 6.33\n",
            "Number of Images for Entire Document: 53\n",
            "Average Images per Section: 0.33\n",
            "Number of Duplicate Links: 835\n"
          ]
        }
      ]
    }
  ]
}